# A Whale off the Port(folio)

## Import the pandas library as pd
import pandas as pd
import numpy as np
import datetime as dt
from pathlib import Path
%matplotlib inline


## Create a path to the File using pathlib.
whale_returns_csvpath = Path("../Resources/whale_returns.csv")
algo_returns_csvpath = Path("../Resources/algo_returns.csv")
sp500_history_csvpath = Path("../Resources/sp500_history.csv")


## Read the CSV into a Pandas DataFrame.
whale_returns_df = pd.read_csv(whale_returns_csvpath)
algo_returns_df = pd.read_csv(algo_returns_csvpath)
sp500_history_df = pd.read_csv(sp500_history_csvpath)
whale_returns_csv_data = pd.read_csv(whale_returns_csvpath)
algo_returns_csv_data = pd.read_csv(algo_returns_csvpath)
sp500_history_csv_data = pd.read_csv(sp500_history_csvpath)


whale_returns_df.columns


## Whale Returns
# Read the CSV 
whale_returns_df = pd.read_csv(whale_returns_csvpath)
whale_returns_df


# Identify the number of rows and columns (shape)
whale_returns_csv_data.shape


# Identify the number of records in the Whale_Returns DataFrame, and compare it with the number of rows in the original file.
whale_returns_csv_data.count() 


## Count nulls
# Identify nulls records in Whale_Returns DataFrame.
whale_returns_csv_data.isnull().sum()


## Drop Nulls
# Drop Null Records in Whale_Returns DataFrame.
whale_returns_csv_data = whale_returns_csv_data.dropna()#.copy()
whale_returns_csv_data


# Validate nulls have been dropped in Whale_Returns DataFrame.
whale_returns_csv_data.isnull().sum()


# Assess data quality by checking for duplicate rows
whale_returns_csv_data.duplicated()


## Algo Daily Returns 
# Read the CSV
algo_returns_df = pd.read_csv(algo_returns_csvpath)
algo_returns_df


## Count nulls
# Identify nulls records in Algo_Returns DataFrame.
algo_returns_csv_data.isnull().sum()


## Drop nulls
# Drop Null Records in Algo_Returns DataFrame.
algo_returns_csv_data = algo_returns_csv_data.dropna()#.copy()
algo_returns_csv_data


# Validate nulls have been dropped in Algo_Returns DataFrame.
algo_returns_csv_data.isnull().sum()


# Assess data quality by checking for duplicate rows
algo_returns_csv_data.duplicated()


# S&P 500 Returns
## Read the CSV 
# Read the CSV into a sp500_history DataFrame.
sp500_history_df = pd.read_csv(sp500_history_csvpath)
sp500_history_df


## Check Data Types
# Identify DataFrame Data Types in sp500_history DataFrame.
# Retrieve DataFrame data types
sp500_history_csv_data.dtypes


# Assess data quality by identifying the number of rows in sp500_history DataFrame.
# Identify Series count
sp500_history_csv_data.count()


# Assess data quality by identifying the number of times a value occurs in sp500_history DataFrame.
# Identify frequency of values
sp500_history_csv_data["Date"].value_counts()


## Calculate Daily Returns 
# Calculate the daily return 
daily_returns = sp500_history_df
daily_returns.head()


## Drop Nulls
# Assess data quality by checking for nulls in sp500_history DataFrame.
sp500_history_csv_data.isnull().sum()


# Drop Null Records in sp500_history DataFrame.
sp500_history_csv_data = sp500_history_csv_data.dropna()#.copy()
sp500_history_csv_data


# Validate nulls have been dropped in sp500_history DataFrame.
sp500_history_csv_data.isnull().sum()


## Rename Column in sp500_history DataFrame.
# Use the `rename` function and set the `columns` parameter to a dictionary of new column names
sp500_history_df = sp500_history_df.rename(columns={
    "Date": "Date",
    "Close": "Daily Close",
})

sp500_history_df.head()


# Combine Whale, Algorithmic, and S&P 500 Returns 
# Join Whale Returns, Algorithmic Returns, and the S&P 500 Returns into a single DataFrame with columns for each portfolio's returns.
# Concatenate the three DataFrames by columns and perform an inner join
portfolio_appended_data = pd.concat([whale_returns_df, algo_returns_df, sp500_history_df], axis="columns", join="inner")
portfolio_appended_data.head()



# Create a new pivot table where the columns are the closing prices for each ticker
combined_df = pd.concat([whale_returns_df, algo_returns_df, sp500_history_df], axis="columns", join="inner")

# Sort datetime index in ascending order (past to present)
combined_df.sort_index(inplace=True)

# Set column names
combined_df.columns = ['SOROS FUND MANAGEMENT LLC', 'PAULSON & CO.INC.', 'TIGER GLOBAL MANAGEMENT LLC', 'BERKSHIRE HATHAWAY INC', 'Algo 1', 'Algo 2', 'Close']

# Display a few rows
combined_df.head()


# Display the current column names
portfolio_appended_data.columns


# Change row values within the 'Algo 2' column from NaN to 'Unknown'
portfolio_appended_data.loc[algo_returns_df['Algo 2'].isnull(), 'Algo 2'] = 'Unknown'
portfolio_appended_data.head()


# Conduct Quantitative Analysis

## Calculate the daily returns of all portfolios
daily_returns = column_appended_data.dropna()
daily_returns


# Display summary statistics for all portfolios
daily_returns.describe()


# Display chart
daily_returns.plot()


# Calculate and Plot cumulative returns.
# Use the `cumprod` function to calculate cumulative returns
cumulative_returns = (1 + daily_returns).cumprod()
cumulative_returns.head()

# Plot cumulative returns
cumulative_returns.plot()


# Risk Analysis
# Evaluate Riskiness of Stocks
# Create a box plot for whale_returns.
whale_returns_df.plot.box()

# Create a box plot for algo_returns.
algo_returns_df.plot.box()

# Use the `std` function and multiply by the square root of the number of trading days in a year to get annualized volatility
volatility = daily_returns.std() * np.sqrt(252)
volatility.sort_values(inplace=True)
volatility


# Use the `std` function and multiply by the square root of the number of trading days in a year to get annualized volatility
volatility = sp500_history_df.std() * np.sqrt(252)
volatility.sort_values(inplace=True)
volatility


# Rolling Statistics
# Calculate and plot rolling std for all portfolios with 21-day window
# Calculate a rolling 21-day std dev of whale portfolio
whale_returns_df.rolling(window=21, axis=0).std()

# Plot a rolling 21-day std dev of whale portfolio
whale_returns_df.rolling(window=21).std().plot()


# Calculate a rolling 21-day std dev of algo portfolio
algo_returns_df.rolling(window=21, axis=0).std()

# Plot a rolling 21-day std dev of algo portfolio
algo_returns_df.rolling(window=21).std().plot()


# Plot a rolling 21-day std dev of sp500 closing prices
sp500_history_df.rolling(window=21).std().plot()


# Import the Seaborn library
import seaborn as sns

# Calculate Correlation
# Use the `corr` function to calculate correlations for all portfolios
correlation = combined_returns.corr()
correlation


# Create a heatmap from the correlation values
sns.heatmap(correlation, vmin=-1, vmax=1)


# Calculate and Plot Beta for a chosen portfolio and the S&P 500
# Calculate covariance of a single portfolio
covariance = daily_returns['Algo 1'].cov(daily_returns['Close'])
covariance = daily_returns['Algo 2'].cov(daily_returns['Close'])
covariance


# Calculate Variance of S&P 500 Returns
# Calculate variance of all daily returns of portfolios vs. S&P 500
variance = daily_returns['S&P 500'].var()
variance


# Calculate Beta Values of portfolio
Algo 1_beta = Algo 1_covariance / variance
Algo 2_beta = Algo 2_covariance / variance

print(f"Algo 1: {Algo 1_beta} | Algo 2: {Algo 2_beta}")


# Set the figure and plot the different datasets as multiple trends on the same figure
ax = rolling_Algo 1_beta.plot(figsize=(20, 10), title='Rolling 21-Day Beta of Portfolio')
rolling_Algo 2_beta.plot(ax=ax)

# Set the legend of the figure
ax.legend(["Algo 1", "Algo 2"])


# Rolling Statistics Challenge: Exponentially Weighted Average
whale_returns_df.ewm(halflife=21).mean()


